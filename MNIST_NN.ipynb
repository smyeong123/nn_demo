{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ab2279-d39e-4556-9d04-1318ee9c6673",
   "metadata": {},
   "source": [
    "# MNIST Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de03bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 13:43:56.652851: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import os, matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34848dca-6c4b-4b54-8ec8-2ca47c99f161",
   "metadata": {},
   "source": [
    "## Generate Pickle File from Tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f84a614-53a3-4696-8e34-40555fca1081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST data saved as mnist.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load MNIST data\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# Create a dictionary with all the data\n",
    "mnist_data = {\n",
    "    'train_X': train_X,\n",
    "    'train_y': train_y,\n",
    "    'test_X': test_X,\n",
    "    'test_y': test_y\n",
    "}\n",
    "\n",
    "# Save as pickle file\n",
    "with open('datasets/mnist.pickle', 'wb') as f:\n",
    "    pickle.dump(mnist_data, f)\n",
    "\n",
    "print(\"MNIST data saved as mnist.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049acca5-f4d6-4215-92c5-6fbbba68cd5f",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353d4207-e306-43fa-a6cc-4016710b69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "    \n",
    "    return (train_X, train_y), (test_X, test_y)\n",
    "    \n",
    "def get_data():\n",
    "    (train_X, train_y), (test_X, test_y) = load_data()\n",
    "    \n",
    "    # Flatten the images (reshape from 28x28 to 784 features)\n",
    "    train_X_flat = train_X.reshape(train_X.shape[0], -1)\n",
    "    test_X_flat = test_X.reshape(test_X.shape[0], -1)\n",
    "    \n",
    "    # Normalize pixel values to range [0, 1]\n",
    "    train_X_normalized = train_X_flat / 255.0\n",
    "    test_X_normalized = test_X_flat / 255.0\n",
    "\n",
    "    # print(train_X_normalized.shape)\n",
    "    # print(train_y.shape)\n",
    "    # print(test_X_normalized.shape)\n",
    "    # print(test_y.shape)\n",
    "    \n",
    "    return test_X_normalized, test_y\n",
    "    \n",
    "def init_network():\n",
    "    with open('datasets/sample_weight.pkl', 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff5df98-2e37-4a9a-a39e-2a7091a0972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2) \n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067602ec-8f7a-4c63-8cd2-edaf34b96fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n",
      "Original shape: (28, 28)\n",
      "Reshaped to: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def img_show(img):\n",
    "    # Scale back to 0-255 range for display \n",
    "    img_255 = (img * 255).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(img_255)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample\n",
    "(train_X, train_y), (test_X, test_y) = load_data()\n",
    "img = train_X[0]\n",
    "label = train_y[0]\n",
    "print(f\"Label: {label}\")\n",
    "\n",
    "print(f\"Original shape: {img.shape}\")\n",
    "img = img.reshape(28, 28)\n",
    "print(f\"Reshaped to: {img.shape}\")\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebc2d0-22f0-42fb-b42a-97946f98ecef",
   "metadata": {},
   "source": [
    "## Accuracy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae333dd-122e-43ea-aa2b-aaffcf9cbc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "test_X, test_y = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(test_X)):\n",
    "    y = predict(network, test_X[i])\n",
    "    p = np.argmax(y)\n",
    "    if p == test_y[i]:\n",
    "        accuracy_cnt += 1\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a403-6ab9-40b1-8f2b-7cb8252408d9",
   "metadata": {},
   "source": [
    "## Batch Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46ea707-3d34-4ec9-957f-de45df852b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(784,)\n",
      "(784, 50)\n",
      "(50, 100)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "# The dimension of the data\n",
    "x, _ = get_data()\n",
    "network = init_network()\n",
    "W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "print(W3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc8c18c-714a-44b8-9caa-5a545f416841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(test_x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == test_y[i:i+batch_size])\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5b306-5a48-4f83-b285-c8f1e62b7f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8797f90f-b25a-4522-80fe-0777e07761ad",
   "metadata": {},
   "source": [
    "## Sample NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa8c21-b531-4b23-b1d5-82eb3a3ef65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the uploaded image\n",
    "image_path = \"9.png\"\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Preprocessing: Resize, Invert, Normalize\n",
    "img = cv2.resize(img, (28, 28))\n",
    "img = cv2.bitwise_not(img)  # Invert colors if needed\n",
    "img = img / 255.0  # Normalize\n",
    "\n",
    "# Display the processed image\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(\"Processed Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Reshape for model input\n",
    "img = img.reshape(1, 28, 28)  # Model expects (1, 28, 28)\n",
    "img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe87b5-983b-47c6-a9c8-a0b0e4fec6a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "\n",
    "def initialize_cnn(input_shape=(28, 28, 1)):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Calculate feature map sizes\n",
    "    conv1_output = (input_shape[0] - 3 + 1, input_shape[1] - 3 + 1, 32)\n",
    "    pool1_output = (conv1_output[0] // 2, conv1_output[1] // 2, 32)\n",
    "    conv2_output = (pool1_output[0] - 3 + 1, pool1_output[1] - 3 + 1, 64)\n",
    "    pool2_output = (conv2_output[0] // 2, conv2_output[1] // 2, 64)\n",
    "    flattened_size = pool2_output[0] * pool2_output[1] * pool2_output[2]\n",
    "    \n",
    "    params = {\n",
    "        \"W1\": np.random.randn(3, 3, 1, 32) * 0.01,\n",
    "        \"b1\": np.zeros((1, 1, 1, 32)),\n",
    "        \"W2\": np.random.randn(3, 3, 32, 64) * 0.01,\n",
    "        \"b2\": np.zeros((1, 1, 1, 64)),\n",
    "        \"W3\": np.random.randn(flattened_size, 128) * 0.01,\n",
    "        \"b3\": np.zeros((1, 128)),\n",
    "        \"W4\": np.random.randn(128, 10) * 0.01,\n",
    "        \"b4\": np.zeros((1, 10)),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def convolve(X, W, b):\n",
    "    batch_size, height, width, channels = X.shape\n",
    "    f_height, f_width, in_channels, out_channels = W.shape\n",
    "    out_height = height - f_height + 1\n",
    "    out_width = width - f_width + 1\n",
    "    Z = np.zeros((batch_size, out_height, out_width, out_channels))\n",
    "    \n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            region = X[:, i:i+f_height, j:j+f_width, :]  # Shape: (batch_size, f_height, f_width, in_channels)\n",
    "            # Reshape region to match W for proper broadcasting\n",
    "            region_reshaped = region.reshape(batch_size, f_height, f_width, in_channels, 1)\n",
    "            W_reshaped = W.reshape(1, f_height, f_width, in_channels, out_channels)\n",
    "            # Multiply and sum the appropriate dimensions\n",
    "            Z[:, i, j, :] = np.sum(region_reshaped * W_reshaped, axis=(1, 2, 3)).reshape(batch_size, out_channels) + b\n",
    "    \n",
    "    return Z\n",
    "\n",
    "def max_pool(X, size=2, stride=2):\n",
    "    batch_size, height, width, channels = X.shape\n",
    "    out_height = (height - size) // stride + 1\n",
    "    out_width = (width - size) // stride + 1\n",
    "    pooled = np.zeros((batch_size, out_height, out_width, channels))\n",
    "    \n",
    "    for i in range(out_height):\n",
    "        for j in range(out_width):\n",
    "            region = X[:, i*stride:i*stride+size, j*stride:j*stride+size, :]\n",
    "            pooled[:, i, j, :] = np.max(region, axis=(1, 2))\n",
    "    \n",
    "    return pooled\n",
    "\n",
    "def compute_loss(Y_pred, Y_true):\n",
    "    m = Y_true.shape[0]\n",
    "    return -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
    "\n",
    "def forward_propagation(X, params):\n",
    "    X = X.reshape(-1, 28, 28, 1)\n",
    "    Z1 = convolve(X, params[\"W1\"], params[\"b1\"])\n",
    "    A1 = relu(Z1)\n",
    "    P1 = max_pool(A1)\n",
    "    Z2 = convolve(P1, params[\"W2\"], params[\"b2\"])\n",
    "    A2 = relu(Z2)\n",
    "    P2 = max_pool(A2)\n",
    "    \n",
    "    F = P2.reshape(P2.shape[0], -1)\n",
    "    Z3 = np.dot(F, params[\"W3\"]) + params[\"b3\"]\n",
    "    A3 = relu(Z3)\n",
    "    Z4 = np.dot(A3, params[\"W4\"]) + params[\"b4\"]\n",
    "    A4 = softmax(Z4)\n",
    "    \n",
    "    cache = {\"A1\": A1, \"P1\": P1, \"A2\": A2, \"P2\": P2, \"A3\": A3, \"A4\": A4, \"F\": F}\n",
    "    return A4, cache\n",
    "\n",
    "def backward_propagation(X, Y, params, cache):\n",
    "    m = X.shape[0]\n",
    "    X = X.reshape(-1, 28, 28, 1)\n",
    "    A1, P1, A2, P2, A3, A4, F = cache[\"A1\"], cache[\"P1\"], cache[\"A2\"], cache[\"P2\"], cache[\"A3\"], cache[\"A4\"], cache[\"F\"]\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dZ4 = A4 - Y\n",
    "    dW4 = np.dot(A3.T, dZ4) / m\n",
    "    db4 = np.sum(dZ4, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Gradients for hidden fully connected layer\n",
    "    dA3 = np.dot(dZ4, params[\"W4\"].T)\n",
    "    dZ3 = dA3 * relu_derivative(A3)\n",
    "    dW3 = np.dot(F.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # For this simplified implementation, we won't backpropagate through\n",
    "    # the convolutional layers as it's computationally intensive\n",
    "    # In a complete implementation, you would compute dW1, db1, dW2, db2 here\n",
    "    \n",
    "    gradients = {\"dW3\": dW3, \"db3\": db3, \"dW4\": dW4, \"db4\": db4}\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(params, gradients, learning_rate):\n",
    "    for key in gradients.keys():\n",
    "        param_key = key[1:]  # Remove the 'd' prefix to get the parameter key\n",
    "        params[param_key] -= learning_rate * gradients[key]\n",
    "    return params\n",
    "\n",
    "def train_cnn(X, Y, params, learning_rate=0.01, epochs=10):\n",
    "    m = X.shape[0]\n",
    "    Y_one_hot = np.eye(10)[Y]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        Y_pred, cache = forward_propagation(X, params)\n",
    "        loss = compute_loss(Y_pred, Y_one_hot)\n",
    "        gradients = backward_propagation(X, Y_one_hot, params, cache)\n",
    "        \n",
    "        # Only update parameters we have gradients for\n",
    "        params[\"W3\"] -= learning_rate * gradients[\"dW3\"]\n",
    "        params[\"b3\"] -= learning_rate * gradients[\"db3\"]\n",
    "        params[\"W4\"] -= learning_rate * gradients[\"dW4\"]\n",
    "        params[\"b4\"] -= learning_rate * gradients[\"db4\"]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"File not found: {image_path}\")\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Failed to load image. Check file format and path.\")\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = cv2.bitwise_not(img)\n",
    "    img = img / 255.0\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    "    return img\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = train_X / 255.0\n",
    "test_X = test_X / 255.0\n",
    "\n",
    "params = initialize_cnn()\n",
    "train_cnn(train_X[:1000], train_y[:1000], params, epochs=10)\n",
    "\n",
    "def predict_image(image_path, params):\n",
    "    try:\n",
    "        img = preprocess_image(image_path)\n",
    "        prediction, _ = forward_propagation(img, params)\n",
    "        return np.argmax(prediction, axis=1)[0]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example usage\n",
    "image_path = '4.png'  # Change to the uploaded image path\n",
    "predicted_number = predict_image(image_path, params)\n",
    "print(f'Predicted number: {predicted_number}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcecd8e-e68c-46d4-8512-7094d3b42eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '9.png'  # Change to the uploaded image path\n",
    "predicted_number = predict_image(image_path, params)\n",
    "print(f'Predicted number: {predicted_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b87aa-0286-47b0-8c8d-365657a1df04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
