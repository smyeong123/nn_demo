{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ff6508-6e6f-4cc6-b288-5c3ad7115909",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9166fa-6fb9-4bb8-9e2c-2b073178bfe4",
   "metadata": {},
   "source": [
    "## Neural Network Training Process\n",
    "### Data Preparation\n",
    "Collect and preprocess the data (e.g., normalization, cleaning).\n",
    "\n",
    "### Forward Propagation\n",
    "Input data is passed through the network layer by layer to compute the output.\n",
    "Each neuron applies an activation function (e.g., ReLU, Sigmoid) to transform the input signal.\n",
    "\n",
    "### Loss Calculation\n",
    "The difference between the network’s prediction and the actual output (label) is measured using a loss function (e.g., Mean Squared Error, Cross-Entropy Loss).\n",
    "\n",
    "### Backpropagation\n",
    "The error is propagated backward through the network to calculate the gradients of the weights and biases.\n",
    "This is done using the chain rule of differentiation to compute partial derivatives.\n",
    "\n",
    "### Weight Update\n",
    "Using the calculated gradients, an optimizer (e.g., SGD, Adam, RMSprop) updates the weights and biases to minimize the loss.\n",
    "\n",
    "### Iteration\n",
    "The process is repeated multiple times, improving the model’s accuracy over time.\n",
    "The term epoch refers to one complete pass through the entire dataset during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8c649-c0a9-4a78-8603-08548e253ad5",
   "metadata": {},
   "source": [
    "## Layers Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6dff17-4ab0-400e-83fb-04d437668a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "\n",
    "    def backward(self, x, y):\n",
    "        return x*1, y*1\n",
    "        \n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x;\n",
    "        self.y = y;\n",
    "        return x*y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "        \n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.cop\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx\n",
    "        \n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # loss function\n",
    "        self.y = None # softmax output\n",
    "        self.t = None # answer layer (one-hot encoded vector)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.y)\n",
    "        return self.loss\n",
    "        \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c943f85-37a4-427a-96d9-49c6818456f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
